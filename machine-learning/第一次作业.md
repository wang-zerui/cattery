<div class="cover" style="page-break-after:always;font-family:方正公文仿宋;width:100%;height:100%;border:none;margin: 0 auto;text-align:center;">
    <div style="width:60%;margin: 0 auto;height:0;padding-bottom:10%;">
        </br>
        <img src="https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2021/11/02/ebdb8ee3007cb2de3e0a4f09817a9812-%E8%A5%BF%E7%94%B5%E6%96%B0%E6%A0%87%E5%BF%974-%E9%BB%91%E8%89%B2-%E9%80%8F%E6%98%8E-a4c8f4.png" alt="校名" style="width:100%;"/>
    </div>
    </br></br></br></br></br>
    <div style="width:60%;margin: 0 auto;height:0;padding-bottom:40%;">
        <img src="https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2021/11/02/189c51c41435ccce69124ca34d09462b-%E8%A5%BF%E7%94%B5%E6%96%B0%E6%A0%87%E5%BF%971-%E7%BA%A2%E8%89%B2-%E9%80%8F%E6%98%8E-781794.png" alt="校徽" style="width:100%;"/>
	</div>
    </br></br></br></br></br></br></br></br>
    <span style="font-family:华文黑体Bold;text-align:center;font-size:20pt;margin: 10pt auto;line-height:30pt;">机器学习与数据挖掘第一次作业</span>
    <p style="text-align:center;font-size:14pt;margin: 0 auto">实验报告 </p>
    </br>
    </br>
    <table style="border:none;text-align:center;width:72%;font-family:仿宋;font-size:14px; margin: 0 auto;">
    <tbody style="font-family:方正公文仿宋;font-size:12pt;">
    	<tr style="font-weight:normal;"> 
    		<td style="width:20%;text-align:right;">题　　目</td>
    		<td style="width:2%">：</td> 
    		<td style="width:40%;font-weight:normal;border-bottom: 1px solid;text-align:center;font-family:华文仿宋"> 对数几率回归牛顿法和梯度下降法实现</td>     </tr>
    	<tr style="font-weight:normal;"> 
    		<td style="width:20%;text-align:right;">上课时间</td>
    		<td style="width:2%">：</td> 
    		<td style="width:40%;font-weight:normal;border-bottom: 1px solid;text-align:center;font-family:华文仿宋"> 2022年3月</td>     </tr>
    	<tr style="font-weight:normal;"> 
    		<td style="width:20%;text-align:right;">授课教师</td>
    		<td style="width:2%">：</td> 
    		<td style="width:40%;font-weight:normal;border-bottom: 1px solid;text-align:center;font-family:华文仿宋">方厚章 </td>     </tr>
    	<tr style="font-weight:normal;"> 
    		<td style="width:20%;text-align:right;">姓　　名</td>
    		<td style="width:2%">：</td> 
    		<td style="width:40%;font-weight:normal;border-bottom: 1px solid;text-align:center;font-family:华文仿宋"> 王泽睿</td>     </tr>
    	<tr style="font-weight:normal;"> 
    		<td style="width:20%;text-align:right;">学　　号</td>
    		<td style="width:2%">：</td> 
    		<td style="width:40%;font-weight:normal;border-bottom: 1px solid;text-align:center;font-family:华文仿宋">19030500132 </td>     </tr>
    	<tr style="font-weight:normal;"> 
    		<td style="width:20%;text-align:right;">年　　级</td>
    		<td style="width:%">：</td> 
    		<td style="width:40%;font-weight:normal;border-bottom: 1px solid;text-align:center;font-family:华文仿宋"> 2019</td>     </tr>
    	<tr style="font-weight:normal;"> 
    		<td style="width:20%;text-align:right;">日　　期</td>
    		<td style="width:2%">：</td> 
    		<td style="width:40%;font-weight:normal;border-bottom: 1px solid;text-align:center;font-family:华文仿宋">2022年3月24日</td>     </tr>
    </tbody>              
    </table>
</div>





<!-- 注释语句：导出PDF时会在这里分页 -->

## 实验内容

使用Python实现牛顿法和梯度下降法求解对率回归（Logisitc Regression）(《机器学习》教材第三章课后习题3.3)。

## 实验环境

1. 操作系统：Macos 12.3 arm64
2. Python：Python3.9.4

## 理论推导

### 模型推导

在二分类任务中，输出标记$$y\in\{0,1\}$$，线性回归模型产生的预测值是连续的实值，为了将实值转换为0/1值，需要用到“单位越阶函数”
$$
y=\left\{\begin{array}{cc}
0, & z<0 \\
0.5, & z=0 \\
1, & z>0
\end{array}\right.
$$
即，若预测值大于0，就判为正例；若预测值小于0，就判为负例；临界值处，任意判别。

由于阶跃函数不可导，不连续，而$g^-(.)$必须是一个可微的函数，所以需要还需要找一个连续函数代替阶跃函数。我们常用**对数几率函数**
$$
y=\frac{1}{1+e^{-z}}
$$
带入广义线性模$$y=g^{-1}\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)$$可以得到
$$
y=\frac{1}{1+e^{-\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)}}
$$
化简可得
$$
\ln\frac{y}{1-y}=\boldsymbol{\omega^{\text{T}}x}+b
$$
$y$是样本为正例的可能性，$\frac{y}{1-y}$称为**几率**，反映$\boldsymbol{x}$是正例的相对可能性，对**几率取对数**得到的就是**对数几率**
$$
\ln \frac{y}{1-y}
$$
式（3）实际上就是在用<u>线性回归模型的预测结果去逼近真实编辑的**对数几率**</u>。因此，其对应模型称为“对数几率回归”

### 确定$\boldsymbol{\omega}$和$b$

将式(4)重写为
$$
\ln \frac{p(y=1 \mid \boldsymbol{x})}{p(y=0 \mid \boldsymbol{x})}=\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b
$$

显然有

$$
\begin{aligned}
&p(y=1 \mid \boldsymbol{x})=\frac{e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}} \\
&p(y=0 \mid \boldsymbol{x})=\frac{1}{1+e^{\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b}}
\end{aligned}
$$
可以通过**极大似然法**估计$\boldsymbol{\omega}$和$b$，首先进行如下标记
$$
\boldsymbol{\beta}=(\boldsymbol{w} ; b)\\
\hat{\boldsymbol{x}}=(\boldsymbol{x} ; 1)\\
p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})=p(y=1 \mid \hat{\boldsymbol{x}} ; \boldsymbol{\beta})\\
p_{0}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})=p(y=0 \mid \hat{\boldsymbol{x}} ; \boldsymbol{\beta})=1-p_{1}(\hat{\boldsymbol{x}} ; \boldsymbol{\beta})
$$
则似然项可以写为
$$
p\left(y_{i} \mid \boldsymbol{x}_{i} ; \boldsymbol{w}, b\right)=y_{i} p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)+\left(1-y_{i}\right) p_{0}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)
$$
则我们可以得到**似然函数为**
$$
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(y_{i} \boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}-\ln \left(1+e^{\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}}\right)\right)
$$
推导过程如下：

首先
$$
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\ln\left(y_ip_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})+(1-y_i)p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})\right)
$$
其中
$$
p_1(\hat{\boldsymbol x}_i;\boldsymbol{\beta})=\cfrac{e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}}{1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}},p_0(\hat{\boldsymbol x}_i;\boldsymbol{\beta})=\cfrac{1}{1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}}
$$
代入上式可得
$$
\begin{aligned} 
\ell(\boldsymbol{\beta})&=\sum_{i=1}^{m}\ln\left(\cfrac{y_ie^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}+1-y_i}{1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}}\right) \\
&=\sum_{i=1}^{m}\left(\ln(y_ie^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i}+1-y_i)-\ln(1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i})\right) 
\end{aligned}
$$
由于$ y_i $=0或1，则
$$
\ell(\boldsymbol{\beta}) =
\begin{cases} 
\sum_{i=1}^{m}(-\ln(1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i})),  & y_i=0 \\
\sum_{i=1}^{m}(\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i-\ln(1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i})), & y_i=1
\end{cases}
$$
两式综合可得

$$
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(y_i\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i-\ln(1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i})\right)
$$
此时$\ell(\boldsymbol{\beta})$是似然函数，需要求其最大化，等价于最小化
$$
\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(-y_i\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i+\ln(1+e^{\boldsymbol{\beta}^{\mathrm{T}}\hat{\boldsymbol x}_i})\right)
$$
下面可以使用梯度下降法和牛顿法分别求最优解
$$
\boldsymbol{\beta}^{*}=\underset{\boldsymbol{\beta}}{\arg \min } \ell(\boldsymbol{\beta})
$$

## 牛顿法

### 原理

第$t+1$轮的迭代公式为
$$
\boldsymbol{\beta}^{t+1}=\boldsymbol{\beta}^{t}-\left(\frac{\partial^{2} \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^{\mathrm{T}}}\right)^{-1} \frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}
$$
其中一阶二阶导数分别为
$$
\begin{aligned}
\frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} &=-\sum_{i=1}^{m} \hat{\boldsymbol{x}}_{i}\left(y_{i}-p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right) \\
\frac{\partial^{2} \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^{\mathrm{T}}} &=\sum_{i=1}^{m} \hat{\boldsymbol{x}}_{i} \hat{\boldsymbol{x}}_{i}^{\mathrm{T}} p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\left(1-p_{1}\left(\hat{\boldsymbol{x}}_{i} ; \boldsymbol{\beta}\right)\right)
\end{aligned}
$$

### 数据准备

读取csv文件

```python
import numpy as np

data_file = './watermelon_data.csv'
data = np.loadtxt(data_file, skiprows=1, delimiter=",")
```

### 数据处理

```python
X = data[:,1:3]
y = data[:,3:]
X0 = np.ones(np.shape(X)[0])
X = np.c_[X, X0.T]
```

$X$即$\hat{x_i}$，$y$即$y_i$

![image-20220325164717057](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/03/25/eb499059f73fdb37ea8ed206232f0e7a-eb499059f73fdb37ea8ed206232f0e7a-image-20220325164717057-1c4b98-558b07.png)

### 数据集可视化

```python
for i in range(X.shape[0]):
  if y[i, 0] == 0:
    plt.plot(X[i,0], X[i,1], 'r+')
	else:
    plt.plot(X[i,0], X[i,1], 'bo')
```

<img src="https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/03/25/37a42ff967c5559fbfa02bf3b2dcd0dd-image-20220325190710356-0f6db0.png" alt="image-20220325190710356" style="zoom:50%;" />

### 牛顿法实现

主要参考提供的示例

```python
def newton(X, y):
    """
    Input:
        X: np.array with shape [N, 3]. Input.
        y: np.array with shape [N, 1]. Label.
    Return:
        beta: np.array with shape [1, 3]. Optimal params with newton method
    """
    N = X.shape[0]
    beta = np.ones((1, 3))
    z = X.dot(beta.T)
    old_l = 0
    new_l = np.sum(-y*z + np.log( 1+np.exp(z) )
    # 迭代次数
    iters = 0
    # 制定阈值             
    while( np.abs(old_l - new_l) > 1e-5):
        p1 = np.exp(z) / (1 + np.exp(z))
        p = np.diag((p1 * (1-p1)).reshape(N))
				# 一阶
        first_order = -np.sum(X * (y - p1), 0, keepdims=True)
        # 二阶
        second_order = X.T .dot(p).dot(X)
				# beta迭代
        beta -= first_order.dot(np.linalg.inv(second_order))
				# 更新z
        z = X.dot(beta.T)
        old_l = new_l
        new_l = np.sum(-y*z + np.log( 1+np.exp(z) ) )
        iters += 1
    print("牛顿法收敛的迭代次数iters: ", iters)
    print('牛顿法收敛后对应的代价函数值: ', new_l)
    return beta
```

### 求解

```python
beta = newton(X, y)
```

得到

```
牛顿法收敛的迭代次数iters:  6
牛顿法收敛后对应的代价函数值:  8.683660584232863
```

![image-20220325170220727](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/03/25/a73ecce0561ee645d35c4a08af308c9c-image-20220325170220727-e301da.png)

### 结果可视化

在数据集可视化图中绘制直线
$$
\omega_1 x_1+\omega_2 x_2+b=0
$$
直线以上，即满足如下不等式的点
$$
\omega_1 x_1+\omega_2 x_2+b>0
$$
为正例，反之为反例

```python
for i in range(X.shape[0]):
        if y[i, 0] == 0:
            plt.plot(X[i,0], X[i,1], 'r+')
        else:
            plt.plot(X[i,0], X[i,1], 'bo')
plt.plot([0.1, 0.9], [newton_left, newton_right], 'g-', label='Newton method')
plt.legend()

plt.xlabel('density')
plt.ylabel('sugar rate')
plt.title("Logistic Regression")
plt.show()
```

<img src="https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/03/25/08947aa27431a5307ddfa1da01ced20b-08947aa27431a5307ddfa1da01ced20b-image-20220325171039369-682354-2dafbe.png" alt="image-20220325171039369" style="zoom:50%;" />

## 梯度下降算法

### 原理

参考附录B.4，梯度下降算法是常用的一阶优化算法，是求解无约束优化问题最简单、最经典的数值优化算法之一。

考虑无约束优化问题：求解$x$使得$f(x)$最小，其中$f(x)$为连续可微函数，若能构造一个序列$x^0,x^1,x^2,...$满足
$$
f(x^{t+1})<f(x^t), t=0,1,2,....
$$
则不断执行该过程即可收敛到局部最小点，欲满足上式，根据泰勒展开式有
$$
f(x+\Delta x) \simeq f(x)+\Delta x \nabla f(x)
$$
欲满足$f(x+\Delta x)<f(x)$，可选择
$$
\Delta x=-\gamma \nabla f(x)
$$
步长$$\gamma$$是一个小常数。

### 对数几率回归中的应用

似然函数为上面的$f(x)$，一阶导与牛顿法中提到的相同

### python实现

```python
def first_order(z, X, y):
    p1 = np.exp(z) / (1+np.exp(z))
    return -np.sum(X * (y - p1), 0, keepdims=True)

def gradient_descent(X, y, step=0.01, tranning_times=5000):
    step = 0.01
    beta = np.zeros((1,3))
    for i in range(tranning_times):
        z = np.dot(X, beta.T)
        gradient = first_order(z, X ,y)
        beta = beta - step * gradient
    return beta
```

### 运行结果

```python
beta = gradient_descent(X, y)
# 结果：2.51184511,  9.46468379, -3.44947915
```

![image-20220325194453036](https://cdn.jsdelivr.net/gh/xinwuyun/pictures@main/2022/03/25/f4e5506176759ac1056980caa52fa5e0-f4e5506176759ac1056980caa52fa5e0-image-20220325194453036-ee4b37-00d710.png)

# 附录



































